{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":407317,"sourceType":"datasetVersion","datasetId":181273},{"sourceId":215482,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":183713,"modelId":205902},{"sourceId":215488,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":183719,"modelId":205909}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"class Config:#resize images\n    IMAGE_WIDTH = 256  \n    IMAGE_HEIGHT = 256 \n    CHANNELS = 3  \n\n\nimport os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport cv2\nfrom tqdm import tqdm_notebook, tnrange\nfrom glob import glob\nfrom itertools import chain\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\n\nimage_width = Config.IMAGE_WIDTH  \nimage_height = Config.IMAGE_HEIGHT \n\n\nimage_filenames_train = []\nmask_files = glob('kaggle_3m/*/*_mask*')\nfor i in mask_files:\n    image_filenames_train.append(i.replace('_mask', ''))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimage_width = Config.IMAGE_WIDTH  \nimage_height = Config.IMAGE_HEIGHT \n\n\nimage_filenames_train = []\nmask_files = glob('/kaggle/input/lgg-mri-segmentation/kaggle_3m/*/*_mask*')\nfor i in mask_files:\n    image_filenames_train.append(i.replace('_mask', ''))\n\nprint(image_filenames_train[:10])\nlen(image_filenames_train) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#helper functions\ndef visualize_image_mask(rows, columns, image_paths, mask_paths):\n    fig = plt.figure(figsize=(12, 12))\n    for idx in range(1, rows * columns + 1):\n        fig.add_subplot(rows, columns, idx)\n        current_image_path = image_paths[idx]\n        current_mask_path = mask_paths[idx]\n        img = cv2.imread(current_image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        overlay_mask = cv2.imread(current_mask_path)\n        plt.imshow(img)\n        plt.imshow(overlay_mask, alpha=0.4)\n    plt.show()\n\ndef dice_score(true_labels, predicted_labels, smoothing_factor=100):\n    flattened_true = tf.keras.backend.flatten(true_labels)\n    flattened_predicted = tf.keras.backend.flatten(predicted_labels)\n\n    overlap = tf.keras.backend.sum(flattened_true * flattened_predicted)\n    total_area = tf.keras.backend.sum(flattened_true) + tf.keras.backend.sum(flattened_predicted)\n    return (2 * overlap + smoothing_factor) / (total_area + smoothing_factor)\n\ndef dice_coefficients_loss(y_true, y_pred, smooth=100):\n    return -dice_coefficients(y_true, y_pred, smooth)\n\ndef intersection_over_union(true_labels, predicted_labels, smoothing_factor=100):\n    overlap = tf.keras.backend.sum(true_labels * predicted_labels)\n    combined_area = tf.keras.backend.sum(true_labels + predicted_labels)\n    iou_score = (overlap + smoothing_factor) / (combined_area - overlap + smoothing_factor)\n    return iou_score\n\nprint(visualize_image_mask(3, 3 , image_filenames_train, mask_files ))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DataFrame with image and mask paths\ndataset = pd.DataFrame(data={'image_paths': image_filenames_train, 'mask_paths': mask_files})\n\n# Shuffle the dataset\ndf = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Split into training and testing sets\ntrain_data, test_data = train_test_split(dataset, test_size=0.1, random_state=42)\n\n# Further split training data into training and validation sets\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n\n# Print proportions of each split\nprint(f\"Training set: {len(train_data)} samples\")\nprint(f\"Validation set: {len(val_data)} samples\")\nprint(f\"Test set: {len(test_data)} samples\")\n\ntrain_data.shape\ntest_data.shape\nval_data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_generator(\n    image_paths_df,\n    batch_size,\n    aug_params,\n    img_color_mode=\"rgb\",\n    msk_color_mode=\"grayscale\",\n    img_prefix=\"aug_image\",\n    msk_prefix=\"aug_mask\",\n    output_dir=None,\n    img_size=(256, 256),\n    random_seed=1,\n):\n    \"\"\"\n    Generates batches of images and masks with the same transformations applied.\n    \"\"\"\n    img_gen = ImageDataGenerator(**aug_params)\n    msk_gen = ImageDataGenerator(**aug_params)\n\n    img_data_gen = img_gen.flow_from_dataframe(\n        image_paths_df,\n        x_col=\"image_paths\",\n        class_mode=None,\n        color_mode=img_color_mode,\n        target_size=img_size,\n        batch_size=batch_size,\n        save_to_dir=output_dir,\n        save_prefix=img_prefix,\n        seed=random_seed,\n    )\n\n    msk_data_gen = msk_gen.flow_from_dataframe(\n        image_paths_df,\n        x_col=\"mask_paths\",\n        class_mode=None,\n        color_mode=msk_color_mode,\n        target_size=img_size,\n        batch_size=batch_size,\n        save_to_dir=output_dir,\n        save_prefix=msk_prefix,\n        seed=random_seed,\n    )\n\n    combined_gen = zip(img_data_gen, msk_data_gen)\n\n    for (img, mask) in combined_gen:\n        img, mask = normalize_and_diagnose(img, mask)\n        # Optionally visualize the augmentations\n        # visualize_augmentations(img[0], mask[0])\n        yield (img, mask)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_and_diagnose(img, mask):\n    img = img / 255.\n    mask = mask / 255.\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    return(img, mask)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 100\nbatch_size = 32\nlearning_rate = 1e-4\nsmooth = 100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#referred code....\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import (\n    Input,\n    Activation,\n    BatchNormalization,\n    Dropout,\n    Lambda,\n    Conv2D,\n    Conv2DTranspose,\n    MaxPooling2D,\n    concatenate,\n)\n\nfrom tensorflow.keras import backend as K\n\n\ndef unet(input_size=(256, 256, 3)):\n    \n    inputs = Input(input_size)\n\n    # First DownConvolution / Encoder Leg will begin, so start with Conv2D\n    conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(inputs)\n    bn1 = Activation(\"relu\")(conv1)\n    conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(bn1)\n    bn1 = BatchNormalization(axis=3)(conv1)\n    bn1 = Activation(\"relu\")(bn1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n\n    conv2 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(pool1)\n    bn2 = Activation(\"relu\")(conv2)\n    conv2 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(bn2)\n    bn2 = BatchNormalization(axis=3)(conv2)\n    bn2 = Activation(\"relu\")(bn2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n\n    conv3 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(pool2)\n    bn3 = Activation(\"relu\")(conv3)\n    conv3 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(bn3)\n    bn3 = BatchNormalization(axis=3)(conv3)\n    bn3 = Activation(\"relu\")(bn3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n\n    conv4 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(pool3)\n    bn4 = Activation(\"relu\")(conv4)\n    conv4 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(bn4)\n    bn4 = BatchNormalization(axis=3)(conv4)\n    bn4 = Activation(\"relu\")(bn4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(bn4)\n\n    conv5 = Conv2D(filters=1024, kernel_size=(3, 3), padding=\"same\")(pool4)\n    bn5 = Activation(\"relu\")(conv5)\n    conv5 = Conv2D(filters=1024, kernel_size=(3, 3), padding=\"same\")(bn5)\n    bn5 = BatchNormalization(axis=3)(conv5)\n    bn5 = Activation(\"relu\")(bn5)\n\n    \"\"\" Now UpConvolution / Decoder Leg will begin, so start with Conv2DTranspose\n    The gray arrows (in the above image) indicate the skip connections that concatenate the encoder feature map with the decoder, which helps the backward flow of gradients for improved training. \"\"\"\n    up6 = concatenate(\n        [\n            Conv2DTranspose(512, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn5\n            ),\n            conv4,\n        ],\n        axis=3,\n    )\n    \"\"\" After every concatenation we again apply two consecutive regular convolutions so that the model can learn to assemble a more precise output \"\"\"\n    conv6 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(up6)\n    bn6 = Activation(\"relu\")(conv6)\n    conv6 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(bn6)\n    bn6 = BatchNormalization(axis=3)(conv6)\n    bn6 = Activation(\"relu\")(bn6)\n\n    up7 = concatenate(\n        [\n            Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn6\n            ),\n            conv3,\n        ],\n        axis=3,\n    )\n    conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(up7)\n    bn7 = Activation(\"relu\")(conv7)\n    conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(bn7)\n    bn7 = BatchNormalization(axis=3)(conv7)\n    bn7 = Activation(\"relu\")(bn7)\n\n    up8 = concatenate(\n        [\n            Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn7\n            ),\n            conv2,\n        ],\n        axis=3,\n    )\n    conv8 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(up8)\n    bn8 = Activation(\"relu\")(conv8)\n    conv8 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(bn8)\n    bn8 = BatchNormalization(axis=3)(conv8)\n    bn8 = Activation(\"relu\")(bn8)\n\n    up9 = concatenate(\n        [\n            Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn8\n            ),\n            conv1,\n        ],\n        axis=3,\n    )\n    conv9 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(up9)\n    bn9 = Activation(\"relu\")(conv9)\n    conv9 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(bn9)\n    bn9 = BatchNormalization(axis=3)(conv9)\n    bn9 = Activation(\"relu\")(bn9)\n\n    conv10 = Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(bn9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = unet()\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator_param = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\ntrain_gen = train_generator(df_train, batch_size,\n                                train_generator_param,\n                                target_size=(im_height, im_width))\n    \ntest_gen = train_generator(df_val, batch_size,\n                                dict(),\n                                target_size=(im_height, im_width))\n    \nmodel = unet(input_size=(im_height, im_width, 3))\n\nopt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999,amsgrad=False)\n\nmodel.compile(optimizer=opt, loss=dice_coefficients_loss, metrics=[\"binary_accuracy\", iou, dice_coefficients])\n\ncallbacks = [ModelCheckpoint('unet.hdf5', verbose=1, save_best_only=True)]\n\nhistory = model.fit(train_gen,\n                    steps_per_epoch=len(df_train) / batch_size, \n                    epochs=epochs, \n                    callbacks=callbacks,\n                    validation_data = test_gen,\n                    validation_steps=len(df_val) / batch_size)\n\n#instead of running code for 100 epochs i have included pre trained model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}